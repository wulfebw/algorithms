---
overview
---

---
chapter 1,2 - getting started
---
- algorithms 
    - add_binary [CHECK]
    - insertion_sort [CHECK]
    - linear_search [CHECK]
    - merge_sort [CHECK]
    - selection_sort [CHECK]
    - binary_search [CHECK]
    - exercise 2.3-7 two_sum_to [CHECK]
    - problem 2-1 merge_insertion_sort [CHECK]
    - problem 2-2 bubblesort [CHECK]
    - problem 2-4 []
- concepts
    - how to exactly measure time complexity
    - how to 'prove' a function to be correct
    - loop invariance 
    - proving a loop is invariant

---
chapter 2,3 - growth of functions
---
- concepts
    - big theta
    - big O
    - big Omega
    - little o
    - little omega
    - limits (e.g., lhopitals) (for comparing functions you're not sure about e.g., logn vs sqrt(n))
    - different ways of establishing these bounds
        - proof from definition
            - literally finding constants c1, c2, n0 such that the definitions hold
        - lim -> inf
        - proof of theta from bigO and bigOmega
    - stirlings formula for factorials
    - asymptotic notions
    - monotinicity
    - floor and ceiling
    - mod
    - polynomials
    - exp
    - log
        - not that all the log bases are in the same bigTheta
    - factorials
    - iterated factorial
    - iterated log
    - fib

---
chapter 3,4 - divide-and-conquer
---
- algorithms
    - maximum sub-array [CHECK]
    - multiplying matricies [CHECK]
        - normally [CHECK]
        - Strassen’s algorithm []

- concepts
    - recurrences
        - particularly 
            - T(n) = aT(n-b) + f(n)
            - T(n) = aT(n/b) + f(n)
    - how to arrive at T(n) of an equation (and/or a recursive form of it)?
    - solving recurrences, or, determining run times of divide-conquer algorithms
        - substitution
            - pitfalls
        - recursion tree
        - master method
    - proof of master theorem
    - change of variables to allow for applying master theorem
        - e.g., sqrt (this results in log log)
    - maximum subarray problem
    - multiplying matricies
        - proof of Strassen’s algorithm time complexity
    - why do ceiling and floor not matter asymptotically?

---
chapter 4, 5 - Probabilistic Analysis and Randomized Algorithms
---
- algorithms
    - exercise 5.1-2 []
    - exercise 5.1-3 biased_random [CHECK]
    - random permutation [CHECK]
    - unsorted search [CHECK]
    - probabilistic counter [CHECK]

- concepts
    - analyzing cost of an algorithm rather than run time (equivalent analysis)
    - probabilistic analysis
    - randomized algorithms
    - indicator variables (all of these are great probability-based algorithm questions)
        - birthday paradox
        - throwing balls in bins
        - streaks
        - on-line hiring problem

---
chapter 6 - heapsort
---
- algorithms
    - parent, left, right [CHECK]
    - max_heapify, build_max_heap [CHECK]
    - heapsort [CHECK]
    - heap_max, heap_extract_max, heap_increase_key, max_heap_insert [CHECK]

- concepts
    - intro
        - heaps are a data structure that maintains a property over the ordering of its elements with the result that it can quickly access the largest/smallest element
        - there are min heaps (smallest element == root) and max heaps (largest element == root)
        - both share the following properties (these are indicies into the array)
            - parent(i) = i/2       # python: max((i - 1) / 2, 0)
            - left(i) = 2 * i       # python: 2 * i + 1
            - right(i) = 2 * i + 1  # python: 2 * i + 2
        - min heap property is that:
            - A[parent(i)] <= A[i]
        - max heap property is that:
            - A[parent(i)] >= A[i]
        - key operations are:
            - max-heapify
                - maintains heap structure in O(lgn)
            - build-max-head
                - build heap from array in O(n) time
            - heapsort
                - sort inplace in O(nlgn) time
            - max-heap-insert, heap-extract-max, heap-increase-key, heap-maximum running in O(lgn) time enable priority queue
    - maintaining heapiness
        - max/min heap property is maintained by calling max_heapify
            - max_heapify basically looks at a node in the tree and compares it with its left and right children. If one of the children is larger than the parent it swaps the two and then recursively calls max heapify on the smaller element that was swapped
            - basically works by moving the element down the tree
    - building a heap
        - building a heap works by iteratively calling max_heapify on the first half of the elements in an array to be converted into a heap. Only need to call on first half because these are the only elements that will have children
    - heapsort
        - heapsort can be performed inplace with max heap
        - basically, you swap the top (largest) element with the smallest and then remove the last element (which is now the largest). Next, you call max_heapify, which corrects the heap in lgn time. Repeat this until you run out of nodes
    - priority queues
        - priority queues allow for lgn insert / removal of the largest or smallest element of the heap
        - they use the operations of extracting the max and inserting a value

---
chapter 7 - quicksort
---
- algorithms
    - quicksort
    - randomized quicksort

- concepts
    - quicksort is a divide-and-conquer sorting algorithm that works by partitioning an array into two subarrays, the first of which constains elements less than a selected element and the second of which contains elements greater than that element. Quicksort is then recursively run on the two arrays.
    - Worst case is when all the elements are the same or when they are sorted because this give T(n) = T(n-1) + Theta(n) runtime == O(n^2)
    - average case results in an even division of subarrays giving T(n) = 2T(n/2) + Theta(n) == O(nlogn)
    - this requires randomizing the pivot element in some way 
    - the intuition behind why it is fast is that when the subarrays are roughly equally partitioned, many elements (specifically elements in different partitions) are never compared with each other.
    - to implement randomness, switch the pivot element with a random element
    - there are two versions of the partition function. The first is the one presented in clrs. The second is that originally proposed by Hoare. It seems the professor likes Hoares version
    - also note that quicksort runs inplace giving space of O(1)


---
chapter 9 - the selection problem, order statistics, and median
---
- algorithms
    - min, max, simultaneous max/min [CHECK]
    - median in expected linear time [CHECK]
    - median in worst case linear time []

- concepts
    - order statistics are stats about the ith element of the list 
        - ith order stat is the ith smallest element
    - max, min are two special cases
        - it's possible to find the maximum and minimum simultaneously in 3/2 * n rather than what you would expect which is 2*n
    - if we want to find the median of an array, we can do so in O(n^2) worst case and O(n) expected by using an algorithm like quicksort, except that it disregards one half of the array each iteration
    - can also find the median in O(n) worst case time using SELECT 
        - select breaks the elements into groups of 5, sorts them using insertion sort and then calls partition on the medians of those groups

---
chapter 10 - elementary data structures
---
- algorithms
    - stack [CHECK]
    - queue [CHECK]
    - deque [CHECK]
    - stack via 2 queues, queue via 2 stacks [CHECK]

- concepts
    - dynamic sets
        - when operating on data it is often beneficial to store it in some data structure
        - in this case we are dealing with a set of data
        - there are certain operations that are useful and can be performed on that set:
            - search, insert, delete, min, max, successor, predecessor
    - stacks
        - stacks are FILO structure
        - use a top pointer to the element to be removed next
        - push adds element on top of this
        - pop removes current top
    - queues
        - queues are FIFO structure
        - uses head ptr to the next element to remove
        - uses tail ptr to next position to add an element
        - implemented in a few ways, but commonly circular buffer
            - uses trick where you leave a space empty to allow for differentiating full vs empty
        - enqueue adds element to tail
        - dequeue remove element from head
        - in both cases the head/tail value is incremented and then mod by the size of the queue
    - linked list
        - represent items as nodes in a list that point to each other
            - if you have a prev and next, then doubly linked
            - if just next then singly linked
            - have a head -> item without a predecessor
            - tail -> item without a successor
    - trees
        - trees exist and can be represented in a few ways
            1. nodes with pointers to parents and left and right children
            2. nodes with array of children
            3. nodes with left-child pointer and right-sibling pointer

---
chapter 11 - hash tables
---
- algorithms

- concepts
    - intro
        - hash tables implement the dictionary interface in O(1) average / expected time / amortized?
        - they do this by taking advantage of the fact that you can index into an array in O(1) time
        - normally the usefulness of this is limited by the fact that we don't know the index of an element in an array
        - to get around this hash tables use information about that object to compute the index
        - the negative of this is that keys / indicies might not be unique, fortunately this can be made rare and therefore provide for really fast lookup, setting, existence
    - direct address tables
        - start out with just an array in which values are stored in locations indexed by their key, those values are generally just pointers to the relevant object
        - O(1) insert, delete, index
    - hash table
        - you have a universe of keys
        - these are great because you have O(1) set, get, delete
        - but they are bad because that require a huge array to store correctly
        - so instead, map all of the keys k to some subset of values using a hash function, and use the resulting hash as a key into a smaller table
        - this gives collisions, but you get the solid performance by trading off worst case performance for average case performance
        - collisions
            - can use chaining, which is just a linked list in the location holding the values
            - even with collisions and chaining you still get O(1) average case performance
                - you can get O(1) worst case insert and delete and O(1) average case get
            - proving average case depends on assumption of unifrom hashing
                - says keys uniformly distriibuted by hash function accross hash table

---
chapter 12 - binary search trees
---
- algorithms

- concepts 
    - intro
        - bst is a data structure that stores values in such a way that basic operations depend on the tree heigh which is on average O(lgn)
        - implements the following operations: search, min, max, pred, succ, insert, delete
        - therefore can be used for both dictionary and priority queue
        - average case lg(n) for all operations (randomly built tree)
    - succ and pred
        - succ is the node with the key that is the smallest one larger than the current node
            - so, with unique keys, 6 is the succ to 5
        - successor has two cases
            1. the node has a right subtree - return the minimum of that right subtree
            2. no right subtree
                - all left children will be smaller
                - so we have to go up the tree
                - going up, if we are going to the left then those values will be less than the original value
                - so as long as we can keep going left, keep going left (we know this by checking if the new parent's right child is where we came from)
                - once we go up _and_ right, then we know that the current parent node is greater than the original node (because the original node was left of it), so return the current parent
                - it's possible we get to the top of the tree coming from the right. in this case there is no successor b/c we started with the largest element
        - pred is the opposite case
            - for a given node without left children, it's predecessor will be the largest node smaller than it
            - going up and to the right, we know these nodes must be larger than the original (o/w breaks bst property)
            - but once we go up and to the left then we've hit the first node that is smaller than the original node, so we return that

---
chapter 13 - red black trees
---
- algorithms

- concepts 
    - intro 
        - red black tree is a bst that guarantees balance
        - add a color bit to each node
        - establish and enforce some properties on the the tree
            1. every node is either red or black
            2. root is black
            3. every leaf is black
            4. if a node is red then children are black
            5. for each node, each path to leafs contain the same number of black nodes
        - define sentinel used for the parent of the root and leafs, sentinel is black and other fields don't matter - used to save space
        - black height of a node is the number of black nodes from a lnode to a leaf (which is constant for both children of the node and is therefore well defined)
    - why are r-b trees useful?
        - lemma: r-b tree with n nodes has height at most 2lg(n+1)
        

