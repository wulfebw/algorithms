---
overview
---

---
chapter 1,2 - getting started
---
- algorithms 
    - add_binary [CHECK]
    - insertion_sort [CHECK]
    - linear_search [CHECK]
    - merge_sort [CHECK]
    - selection_sort [CHECK]
    - binary_search [CHECK]
    - exercise 2.3-7 two_sum_to [CHECK]
    - problem 2-1 merge_insertion_sort [CHECK]
    - problem 2-2 bubblesort [CHECK]
    - problem 2-4 []
- concepts
    - how to exactly measure time complexity
    - how to 'prove' a function to be correct
    - loop invariance 
    - proving a loop is invariant

---
chapter 2,3 - growth of functions
---
- concepts
    - big theta
    - big O
    - big Omega
    - little o
    - little omega
    - limits (e.g., lhopitals) (for comparing functions you're not sure about e.g., logn vs sqrt(n))
    - different ways of establishing these bounds
        - proof from definition
            - literally finding constants c1, c2, n0 such that the definitions hold
        - lim -> inf
        - proof of theta from bigO and bigOmega
    - stirlings formula for factorials
    - asymptotic notions
    - monotinicity
    - floor and ceiling
    - mod
    - polynomials
    - exp
    - log
        - not that all the log bases are in the same bigTheta
    - factorials
    - iterated factorial
    - iterated log
    - fib

---
chapter 3,4 - divide-and-conquer
---
- algorithms
    - maximum sub-array [CHECK]
    - multiplying matricies [CHECK]
        - normally [CHECK]
        - Strassen’s algorithm []

- concepts
    - recurrences
        - particularly 
            - T(n) = aT(n-b) + f(n)
            - T(n) = aT(n/b) + f(n)
    - how to arrive at T(n) of an equation (and/or a recursive form of it)?
    - solving recurrences, or, determining run times of divide-conquer algorithms
        - substitution
            - pitfalls
        - recursion tree
        - master method
    - proof of master theorem
    - change of variables to allow for applying master theorem
        - e.g., sqrt (this results in log log)
    - maximum subarray problem
    - multiplying matricies
        - proof of Strassen’s algorithm time complexity
    - why do ceiling and floor not matter asymptotically?

---
chapter 4, 5 - Probabilistic Analysis and Randomized Algorithms
---
- algorithms
    - exercise 5.1-2 []
    - exercise 5.1-3 biased_random [CHECK]
    - random permutation [CHECK]
    - unsorted search [CHECK]
    - probabilistic counter [CHECK]

- concepts
    - analyzing cost of an algorithm rather than run time (equivalent analysis)
    - probabilistic analysis
    - randomized algorithms
    - indicator variables (all of these are great probability-based algorithm questions)
        - birthday paradox
        - throwing balls in bins
        - streaks
        - on-line hiring problem

---
chapter 7 - quicksort
---
- algorithms
    - quicksort
    - randomized quicksort

- concepts
    - quicksort is a divide-and-conquer sorting algorithm that works by partitioning an array into two subarrays, the first of which constains elements less than a selected element and the second of which contains elements greater than that element. Quicksort is then recursively run on the two arrays.
    - Worst case is when all the elements are the same or when they are sorted because this give T(n) = T(n-1) + Theta(n) runtime == O(n^2)
    - average case results in an even division of subarrays giving T(n) = 2T(n/2) + Theta(n) == O(nlogn)
    - this requires randomizing the pivot element in some way 
    - the intuition behind why it is fast is that when the subarrays are roughly equally partitioned, many elements (specifically elements in different partitions) are never compared with each other.
    - to implement randomness, switch the pivot element with a random element
    - there are two versions of the partition function. The first is the one presented in clrs. The second is that originally proposed by Hoare. It seems the professor likes Hoares version
    - also note that quicksort runs inplace giving space of O(1)
- 

---
chapter 9 - the selection problem, order statistics, and median
---
- algorithms

- concepts

    - 

---
chapter 10 - elementary data structures
---
- algorithms
    - stack [CHECK]
    - queue [CHECK]
    - deque [CHECK]
    - stack via 2 queues, queue via 2 stacks [CHECK]

- concepts
    - dynamic sets
        - when operating on data it is often beneficial to store it in some data structure
        - in this case we are dealing with a set of data
        - there are certain operations that are useful and can be performed on that set:
            - search, insert, delete, min, max, successor, predecessor
    - stacks
        - stacks are FILO structure
        - use a top pointer to the element to be removed next
        - push adds element on top of this
        - pop removes current top
    - queues
        - queues are FIFO structure
        - uses head ptr to the next element to remove
        - uses tail ptr to next position to add an element
        - implemented in a few ways, but commonly circular buffer
            - uses trick where you leave a space empty to allow for differentiating full vs empty
        - enqueue adds element to tail
        - dequeue remove element from head
        - in both cases the head/tail value is incremented and then mod by the size of the queue
    - linked list
        - represent items as nodes in a list that point to each other
            - if you have a prev and next, then doubly linked
            - if just next then singly linked
            - have a head -> item without a predecessor
            - tail -> item without a successor
    - trees
        - trees exist and can be represented in a few ways
            1. nodes with pointers to parents and left and right children
            2. nodes with array of children
            3. nodes with left-child pointer and right-sibling pointer

---
chapter 11 - hash tables
---
- algorithms

- concepts
    - intro
        - hash tables implement the dictionary interface in O(1) average / expected time / amortized?
        - they do this by taking advantage of the fact that you can index into an array in O(1) time
        - normally the usefulness of this is limited by the fact that we don't know the index of an element in an array
        - to get around this hash tables use information about that object to compute the index
        - the negative of this is that keys / indicies might not be unique, fortunately this can be made rare and therefore provide for really fast lookup, setting, existence
    - direct address tables
        - start out with just an array in which values are stored in locations indexed by their key, those values are generally just pointers to the relevant object
        - O(1) insert, delete, index
    - hash table
        - you have a universe of keys
        - these are great because you have O(1) set, get, delete
        - but they are bad because that require a huge array to store correctly
        - so instead, map all of the keys k to some subset of values using a hash function, and use the resulting hash as a key into a smaller table
        - this gives collisions, but you get the solid performance by trading off worst case performance for average case performance
        - collisions
            - can use chaining, which is just a linked list in the location holding the values
            - even with collisions and chaining you still get O(1) average case performance
                - you can get O(1) worst case insert and delete and O(1) average case get
            - proving average case depends on assumption of unifrom hashing
                - says keys uniformly distriibuted by hash function accross hash table
        - 

