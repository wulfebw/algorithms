---
overview
---

---
getting started
---
- algorithms 
    - add_binary [CHECK]
    - insertion_sort [CHECK]
    - linear_search [CHECK]
    - merge_sort [CHECK]
    - selection_sort [CHECK]
    - binary_search [CHECK]
    - exercise 2.3-7 two_sum_to [CHECK]
    - problem 2-1 merge_insertion_sort [CHECK]
    - problem 2-2 bubblesort [CHECK]
    - problem 2-4 []
- concepts
    - how to exactly measure time complexity
    - how to 'prove' a function to be correct
    - loop invariance 
    - proving a loop is invariant

---
growth of functions
---
- concepts
    - big theta
    - big O
    - big Omega
    - little o
    - little omega
    - limits (e.g., lhopitals) (for comparing functions you're not sure about e.g., logn vs sqrt(n))
    - different ways of establishing these bounds
        - proof from definition
            - literally finding constants c1, c2, n0 such that the definitions hold
        - lim -> inf
        - proof of theta from bigO and bigOmega
    - stirlings formula for factorials
    - asymptotic notions
    - monotinicity
    - floor and ceiling
    - mod
    - polynomials
    - exp
    - log
        - not that all the log bases are in the same bigTheta
    - factorials
    - iterated factorial
    - iterated log
    - fib

---
divide-and-conquer
---
- algorithms
    - maximum sub-array [CHECK]
    - multiplying matricies [CHECK]
        - normally [CHECK]
        - Strassen’s algorithm []

- concepts
    - recurrences
        - particularly 
            - T(n) = aT(n-b) + f(n)
            - T(n) = aT(n/b) + f(n)
    - how to arrive at T(n) of an equation (and/or a recursive form of it)?
    - solving recurrences, or, determining run times of divide-conquer algorithms
        - substitution
            - pitfalls
        - recursion tree
        - master method
    - proof of master theorem
    - change of variables to allow for applying master theorem
        - e.g., sqrt (this results in log log)
    - maximum subarray problem
    - multiplying matricies
        - proof of Strassen’s algorithm time complexity
    - why do ceiling and floor not matter asymptotically?

---
Probabilistic Analysis and Randomized Algorithms
---
- algorithms
    - exercise 5.1-2 []
    - exercise 5.1-3 biased_random [CHECK]
    - random permutation [CHECK]
    - unsorted search [CHECK]
    - probabilistic counter [CHECK]

- concepts
    - analyzing cost of an algorithm rather than run time (equivalent analysis)
    - probabilistic analysis
    - randomized algorithms
    - indicator variables (all of these are great probability-based algorithm questions)
        - birthday paradox
        - throwing balls in bins
        - streaks
        - on-line hiring problem

---
quicksort
---
- quicksort is a divide-and-conquer sorting algorithm that works by partitioning an array into two subarrays, the first of which constains elements less than a selected element and the second of which contains elements greater than that element. Quicksort is then recursively run on the two arrays.
- Worst case is when all the elements are the same or when they are sorted because this give T(n) = T(n-1) + Theta(n) runtime == O(n^2)
- average case results in an even division of subarrays giving T(n) = 2T(n/2) + Theta(n) == O(nlogn)
- this requires randomizing the pivot element in some way 
- the intuition behind why it is fast is that when the subarrays are roughly equally partitioned, many elements (specifically elements in different partitions) are never compared with each other.
- to implement randomness, switch the pivot element with a random element
- there are two versions of the partition function. The first is the one presented in clrs. The second is that originally proposed by Hoare. It seems the professor likes Hoares version
- also note that quicksort runs inplace giving space of O(1)
- 

---
chapter 9 - the selection problem, order statistics, and median
---
- 