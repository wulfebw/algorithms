---
overview
---

---
chapter 1,2 - getting started
---
- algorithms 
    - add_binary [CHECK]
    - insertion_sort [CHECK]
    - linear_search [CHECK]
    - merge_sort [CHECK]
    - selection_sort [CHECK]
    - binary_search [CHECK]
    - exercise 2.3-7 two_sum_to [CHECK]
    - problem 2-1 merge_insertion_sort [CHECK]
    - problem 2-2 bubblesort [CHECK]
    - problem 2-4 []
- concepts
    - how to exactly measure time complexity
    - how to 'prove' a function to be correct
    - loop invariance 
    - proving a loop is invariant

---
chapter 2,3 - growth of functions
---
- concepts
    - big theta
    - big O
    - big Omega
    - little o
    - little omega
    - limits (e.g., lhopitals) (for comparing functions you're not sure about e.g., logn vs sqrt(n))
    - different ways of establishing these bounds
        - proof from definition
            - literally finding constants c1, c2, n0 such that the definitions hold
        - lim -> inf
        - proof of theta from bigO and bigOmega
    - stirlings formula for factorials
    - asymptotic notions
    - monotinicity
    - floor and ceiling
    - mod
    - polynomials
    - exp
    - log
        - not that all the log bases are in the same bigTheta
    - factorials
    - iterated factorial
    - iterated log
    - fib

---
chapter 3,4 - divide-and-conquer
---
- algorithms
    - maximum sub-array [CHECK]
    - multiplying matricies [CHECK]
        - normally [CHECK]
        - Strassen’s algorithm []

- concepts
    - recurrences
        - particularly 
            - T(n) = aT(n-b) + f(n)
            - T(n) = aT(n/b) + f(n)
    - how to arrive at T(n) of an equation (and/or a recursive form of it)?
    - solving recurrences, or, determining run times of divide-conquer algorithms
        - substitution
            - pitfalls
        - recursion tree
        - master method
    - proof of master theorem
    - change of variables to allow for applying master theorem
        - e.g., sqrt (this results in log log)
    - maximum subarray problem
    - multiplying matricies
        - proof of Strassen’s algorithm time complexity
    - why do ceiling and floor not matter asymptotically?
    - inversion
        - to calculate inversions, modify merge sort 
        - divide and conquer algorithms seem to break into (1) go down both sides and recombine or (2) only go down one side
        - this is a go down both sides algorithm
            - when recombining, once you hit a value in the left array that's larger than the rigth array element, you know every element in the left array is larger than that bottom element, so add to the count of inversions the count of the remaining elements in the left array
            - to do it for twice as big, 

---
chapter 4, 5 - Probabilistic Analysis and Randomized Algorithms
---
- algorithms
    - exercise 5.1-2 []
    - exercise 5.1-3 biased_random [CHECK]
    - random permutation [CHECK]
    - unsorted search [CHECK]
    - probabilistic counter [CHECK]

- concepts
    - analyzing cost of an algorithm rather than run time (equivalent analysis)
    - probabilistic analysis
    - randomized algorithms
    - indicator variables (all of these are great probability-based algorithm questions)
        - birthday paradox
        - throwing balls in bins
        - streaks
        - on-line hiring problem

---
chapter 6 - heapsort
---
- algorithms
    - parent, left, right [CHECK]
    - max_heapify, build_max_heap [CHECK]
    - heapsort [CHECK]
    - heap_max, heap_extract_max, heap_increase_key, max_heap_insert [CHECK]

- concepts
    - intro
        - heaps are a data structure that maintains a property over the ordering of its elements with the result that it can quickly access the largest/smallest element
        - there are min heaps (smallest element == root) and max heaps (largest element == root)
        - both share the following properties (these are indicies into the array)
            - parent(i) = i/2       # python: max((i - 1) / 2, 0)
            - left(i) = 2 * i       # python: 2 * i + 1
            - right(i) = 2 * i + 1  # python: 2 * i + 2
        - min heap property is that:
            - A[parent(i)] <= A[i]
        - max heap property is that:
            - A[parent(i)] >= A[i]
        - key operations are:
            - max-heapify
                - maintains heap structure in O(lgn)
            - build-max-head
                - build heap from array in O(n) time
            - heapsort
                - sort inplace in O(nlgn) time
            - max-heap-insert, heap-extract-max, heap-increase-key, heap-maximum running in O(lgn) time enable priority queue
    - maintaining heapiness
        - max/min heap property is maintained by calling max_heapify
            - max_heapify basically looks at a node in the tree and compares it with its left and right children. If one of the children is larger than the parent it swaps the two and then recursively calls max heapify on the smaller element that was swapped
            - basically works by moving the element down the tree
    - building a heap
        - building a heap works by iteratively calling max_heapify on the first half of the elements in an array to be converted into a heap. Only need to call on first half because these are the only elements that will have children
    - heapsort
        - heapsort can be performed inplace with max heap
        - basically, you swap the top (largest) element with the smallest and then remove the last element (which is now the largest). Next, you call max_heapify, which corrects the heap in lgn time. Repeat this until you run out of nodes
    - priority queues
        - priority queues allow for lgn insert / removal of the largest or smallest element of the heap
        - they use the operations of extracting the max and inserting a value

---
chapter 7 - quicksort
---
- algorithms
    - quicksort
    - randomized quicksort

- concepts
    - quicksort is a divide-and-conquer sorting algorithm that works by partitioning an array into two subarrays, the first of which constains elements less than a selected element and the second of which contains elements greater than that element. Quicksort is then recursively run on the two arrays.
    - Worst case is when all the elements are the same or when they are sorted because this give T(n) = T(n-1) + Theta(n) runtime == O(n^2)
    - average case results in an even division of subarrays giving T(n) = 2T(n/2) + Theta(n) == O(nlogn)
    - this requires randomizing the pivot element in some way 
    - the intuition behind why it is fast is that when the subarrays are roughly equally partitioned, many elements (specifically elements in different partitions) are never compared with each other.
    - to implement randomness, switch the pivot element with a random element
    - there are two versions of the partition function. The first is the one presented in clrs. The second is that originally proposed by Hoare. It seems the professor likes Hoares version
    - also note that quicksort runs inplace giving space of O(1)

---
chapter 8 - linear time sort
---
- algorithms
    - counting sort

- concepts
    - intro
        - we've covered a lot of "comparison sort" algorithms
        - these have a lower bound of Omega(nlgn)
        - it's possible to use structure inherit in the input values to sort faster
    - lower bound on comparison sort
        - can view all actions as a1 < a2, but comparisons are a1<a2,a1=a2,a1>a2, etc..
        - can view operation of sorting algorithm as a decision tree where at each node a decision is made concerning the order of two elements i and j
            - for any sorting algorithm that works by comparisons, the leaves of this tree correspond to the possible permutations of the input
            - there are n! permutations, all of them must be there or the algorithm might be incorrect
            - a path from root to leaf denotes a sorting of the input 
                - the length of the path from root to leaf is therefore a lowerbound on the number of comparisons
                - what is the length of the path from root to leaf if the leaves must contain all comparisons?
        - lower bound theorem:
            - Omega(nlgn)
        - proof
            - know that the number of reachabl leaves l must be greater than n! (b/c must contain all permutes)
            - know that a tree of height h can contain at most 2^h leaves
            - so n! < l < 2^h
            - lg(n!) < lg(l) < h
            - therefore l is lowerbounded by lg(n!) = nlgn 
    - counting sort
        - counting sort does the following
            - init array of counts C to 0
            - go through elements in A
                - for each element Aj, increment it's count in C (i.e., C[A[j]]++)
            - start from beginning of C and accumlate total number of elements before each index
                - this puts the number of elements before A[j] in C[A[j]]
            - go through A again, inserting each element into a place in B and decrementing C[A[j]]
        - O(k + n) run time
    - radix sort
        - concept is to sort by 'digits' of a number
        - logically you would want to start with the most significant digit, but radix sort actually starts with the least significant digit
        - move up to most significant digit
        - such that the values become progressively sorted by increasingly digit significance
        - code is   
            - for each digit
                - sort by digit
        - run time is O(d(n+k)) for d digits, n input values, k possible values for each digit
            - requires stable sorting algorithm
        - key thing is to realize that d, the number of digits, cannot depend on n, otherwise this runs in nlgn time
    - bucket sort
        - assume input is uniformly distributed over some range
        - create buckets are regular intervals in that range
        - insert each element into it's bucket based on the value 
        - sort the buckets
        - list the buckets in order
        - e.g.,
            - have a lot of numbers between 0 and 1
            - sort by tenths digit into buckets
            - sort buckets
        - 


---
chapter 9 - the selection problem, order statistics, and median
---
- algorithms
    - min, max, simultaneous max/min [CHECK]
    - median in expected linear time [CHECK]
    - median in worst case linear time []

- concepts
    - order statistics are stats about the ith element of the list 
        - ith order stat is the ith smallest element
    - max, min are two special cases
        - it's possible to find the maximum and minimum simultaneously in 3/2 * n rather than what you would expect which is 2*n
    - if we want to find the median of an array, we can do so in O(n^2) worst case and O(n) expected by using an algorithm like quicksort, except that it disregards one half of the array each iteration
    - can also find the median in O(n) worst case time using SELECT 
        - select breaks the elements into groups of 5, sorts them using insertion sort and then calls partition on the medians of those groups

---
chapter 10 - elementary data structures
---
- algorithms
    - stack [CHECK]
    - queue [CHECK]
    - deque [CHECK]
    - stack via 2 queues, queue via 2 stacks [CHECK]

- concepts
    - dynamic sets
        - when operating on data it is often beneficial to store it in some data structure
        - in this case we are dealing with a set of data
        - there are certain operations that are useful and can be performed on that set:
            - search, insert, delete, min, max, successor, predecessor
    - stacks
        - stacks are FILO structure
        - use a top pointer to the element to be removed next
        - push adds element on top of this
        - pop removes current top
    - queues
        - queues are FIFO structure
        - uses head ptr to the next element to remove
        - uses tail ptr to next position to add an element
        - implemented in a few ways, but commonly circular buffer
            - uses trick where you leave a space empty to allow for differentiating full vs empty
        - enqueue adds element to tail
        - dequeue remove element from head
        - in both cases the head/tail value is incremented and then mod by the size of the queue
    - linked list
        - represent items as nodes in a list that point to each other
            - if you have a prev and next, then doubly linked
            - if just next then singly linked
            - have a head -> item without a predecessor
            - tail -> item without a successor
    - trees
        - trees exist and can be represented in a few ways
            1. nodes with pointers to parents and left and right children
            2. nodes with array of children
            3. nodes with left-child pointer and right-sibling pointer

---
chapter 11 - hash tables
---
- algorithms

- concepts
    - intro
        - hash tables implement the dictionary interface in O(1) average / expected time / amortized?
        - they do this by taking advantage of the fact that you can index into an array in O(1) time
        - normally the usefulness of this is limited by the fact that we don't know the index of an element in an array
        - to get around this hash tables use information about that object to compute the index
        - the negative of this is that keys / indicies might not be unique, fortunately this can be made rare and therefore provide for really fast lookup, setting, existence
    - direct address tables
        - start out with just an array in which values are stored in locations indexed by their key, those values are generally just pointers to the relevant object
        - O(1) insert, delete, index
    - hash table
        - you have a universe of keys
        - these are great because you have O(1) set, get, delete
        - but they are bad because that require a huge array to store correctly
        - so instead, map all of the keys k to some subset of values using a hash function, and use the resulting hash as a key into a smaller table
        - this gives collisions, but you get the solid performance by trading off worst case performance for average case performance
        - collisions
            - can use chaining, which is just a linked list in the location holding the values
            - even with collisions and chaining you still get O(1) average case performance
                - you can get O(1) worst case insert and delete and O(1) average case get
            - proving average case depends on assumption of unifrom hashing
                - says keys uniformly distriibuted by hash function accross hash table

---
chapter 12 - binary search trees
---
- algorithms

- concepts 
    - intro
        - bst is a data structure that stores values in such a way that basic operations depend on the tree heigh which is on average O(lgn)
        - implements the following operations: search, min, max, pred, succ, insert, delete
        - therefore can be used for both dictionary and priority queue
        - average case lg(n) for all operations (randomly built tree)
    - succ and pred
        - succ is the node with the key that is the smallest one larger than the current node
            - so, with unique keys, 6 is the succ to 5
        - successor has two cases
            1. the node has a right subtree - return the minimum of that right subtree
            2. no right subtree
                - all left children will be smaller
                - so we have to go up the tree
                - going up, if we are going to the left then those values will be less than the original value
                - so as long as we can keep going left, keep going left (we know this by checking if the new parent's right child is where we came from)
                - once we go up _and_ right, then we know that the current parent node is greater than the original node (because the original node was left of it), so return the current parent
                - it's possible we get to the top of the tree coming from the right. in this case there is no successor b/c we started with the largest element
        - pred is the opposite case
            - for a given node without left children, it's predecessor will be the largest node smaller than it
            - going up and to the right, we know these nodes must be larger than the original (o/w breaks bst property)
            - but once we go up and to the left then we've hit the first node that is smaller than the original node, so we return that

---
chapter 13 - red black trees
---
- algorithms

- concepts 
    - intro 
        - red black tree is a bst that guarantees balance
        - add a color bit to each node
        - establish and enforce some properties on the the tree
            1. every node is either red or black
            2. root is black
            3. every leaf is black
            4. if a node is red then children are black
            5. for each node, each path to leafs contain the same number of black nodes
        - define sentinel used for the parent of the root and leafs, sentinel is black and other fields don't matter - used to save space
        - black height of a node is the number of black nodes from a lnode to a leaf (which is constant for both children of the node and is therefore well defined)
    - why are r-b trees useful?
        - lemma: r-b tree with n nodes has height at most 2lg(n+1)
    - rotations
        - 2 kinds, left and right
        - do them to maintain red-black property
        - left rotation
            - assume node's right child is not null
            - pivot left so original node is left child of original right child
            - right child's left child is now original nodes right child
        - right rotations
            - assume nodes left child is not null
            - pivort rigth so original node is right child of original left child
            - left childs's right child is now original nodes left child
    - insert
        - there are 6 cases to consider with insert but three are symmetric
        - case 1. the uncle node is red
            - uncle node is the sibling of the parent of the inserted node
            - the reason we consider the uncle node is that this allows for making symmetric changes to the tree that maintain property 5
            - in this case, we make the uncle and the parent black, and make the grandparent red
        - case 2 and 3: uncle node is black
            - in this case we rotate the tree without changing the black height of the tree

---
chapter 14 - augmenting data structures
---
- algorithms

- concepts
    - intro 
        - augmenting data structures is what you do when the textbook / vanilla data structure does not work for your use case
        - a lot of the time, you just need to adapt a data structure for it to be used effectively rather than creating an entirely new one 
    - dynamic order stats
        - previously we covered how to determine the ith largest element in linear time
        - what we can do instead using red-black trees is pay an up-front cost to place them in the tree and then extract the ith order stat in only lgn time
        - to do this we add additional information
            - add the size of the subtree rooted at each node (which is the number of internal nodes including the root node)
            - that is, size is left.size + right.size + 1
        - define method called OS-select that retrieves the ith largest element (in terms of a preorder walk) in lgn time
            - here the general strategy is (1) define a structure that is amenable to the necessary operations (2) define the operation (3) define how to maintain that operation
            - os-select does the following
                - starts at the root
                - checks if i (the order stat) is less than the size of the tree rooted at x (the current node)
                    - if less than size of right goes to the left, if greater then size fo left goes to the right, if equal returns x
        - how to maintain?  
            - on the way down during insert, increment size of each node on path down
            - there are at most two rotations in a red black tree by the way
            - deletion - move up decrementing
            - for rotations there is an equation for swapping the sizes 
    - how to augment in general?
        - 1. choose underlying data structure
        - 2. determine info to maintain
        - 3. verify that maintaining that info is possible
        - 4. dev the new operations
        - for red black trees, there is a theorem that states that you can add any information and so long as it only depends on x, x.left, x.right, then you can maintain that info in lgn time
            - this is b/c a const number of rotations occur
            - and each change must be propogated only to ancestors which takes lgn time to move up the tree
    - interval trees
        - closed intervals here - means edges included
        - properties of two intervals i and i': one of the following holds
            1. i and i' overlap
            2. i is left of i'
            3. i si right of i'
        - implementation
            - add the low point of each interval to each node as well as the high point
            - also add x.max - max endpoint stored in the subtree of x
            - maintenance not given but should be not too difficult

---
chapter 15 - dynammic programming
---
- algorithms

- concepts
    - intro
        - recall divide and conquer - break a problem into _disjoint_ subproblems and combine their solutions
        - dynammic programming focuses on problems where the subproblems are specifically _not_ disjoint
            - this means that solutions to subproblems can be reused to avoid recomputing their solution
            - called "programming" because it stores values in a table (not sure, wha?)
            - typically apply to optimization problems 
                - may have many answers to a problem, so consider an optimal answer rather than the optimal answer
            - four steps
                1. characterize the structure of an optimal solution
                    - serves not only to describe form of solution
                    - but also to confirm that the optimal substructure property holds
                2. _recursively_ define the optimal solution
                3. compute the value of an optimal solution in bottom up fashion
                4. construct the optimal solution from information already computed
    - rod cutting
        - what pieces to cut a metal rod into in order to maximize revenue generated by those pieces
        - can either choose to cut or not cut at each location
            - so taking the perspective of moving left to right along the rod
        - can phrase the problem as a max over recursively defined values:
            reward_n_length_rod = max(reward_n_length_rod (no cuts), r_cut_1 + r_n-1, r_cut_2 + r_n-2, + ... + r_n-1 + r1)
        - the r subscript terms are unknown but we can define them recursively
        - exhibits _optimal substructure_ == optimal solutions consist of solutions to subproblems which are also optimal
        - a simpiler phrasing of the problem is 
            - rather than break it into the different cuts above, start at one end, cut in only one place, and say the first rod is fixed and the second may be divided further, then move along rod
            - gives max 1 <= i <= n (p_i + r_n-i)
                - price of first part and max revenue from second part
                - this now only have one subproblem rather than two
        - this approach naively implemented has exponential runtime because it computes the second value recursively n times and each call does this n times - it's like summing 1 to n, where each index is also a sum from 1 to n
        - we can use DP to make this a polynomial run time algo by storing the computed values and looking them up rather than recomputing
            - so basically we trade memory for time
        - two DP methods
            1. top down method
                - this consists of recursive calls that check for the value they need to compute before they do 
                - when they do compute the value, they store it in the table
                - this is called memoization
                - these consist of recursive calls 
                - generalized policy iteration is this on a general graph
                    - GPI is DP in that it uses previously computed solutions to the values of other states to derive its current value, not exclusively in that it stores its own previous value 
            2. bottom up method
                - this involves ordering the values to be computed from small to large
                - for example, computing value iteration in reverse topological order explicitly solves smaller problems before larger ones
                - this method is generally faster by a constant factor     
        - with DP methods, you'll often compute the optimal values but forget to compute how you got there
            - solution is to also store the solution (and replace it with improvements as you go)
    - matrix chain multiplication
        - what order to multiply matricies in
        - has optimal substructure
        - has overlapping subproblems
        - let's try DP
    - high-level concepts about the application of DP
        - two things: (1) optimal substructure (2) overlapping subproblems
        - optimal substructure
            - optimal solution compossed of optimal solutions to sub problems
            - varies in two ways
                - number of sub problems
                - number of choices of which subproblem to use
                - product of these two generally give the run time 
        - the subproblems, once determined, must be _independent_
            - if they are not then changing one may change the other in a manner that prevents simply storing the result in a table
        - overlapping subproblems
            - subproblems must need to be computed multiple times, and there must be relatively few _distinct_ subproblems (since this is a factor in the runtime)
    - longest common subsequence
        - stopped here

---
chapter 17 - amortized analysis
---
- algorithms

- concepts
    - intro
        - deals with the worst case performance rather than the average case - that's how it differes from average case analysis
        - it considers the average _cost_ in the worst case
    - aggregate Analysis
        - worse case time is T(n), n operations, therefore average cost worst case is T(n) / n
        - does not differentiate between operations
    - accounting method
        - assign credit to each operation
        - track credit
        - can never go negative
        - depends on the amortized cost assigned to each operation
    - potential method
        - define function mapping the state of a data structure at time i to a value of potential energy
        - cost of an operation is c_i
        - then the amortized cost is c_hat_i = c_i + potential after - potential before
        - the sum of amortized costs is then the sum of their true costs + the difference between the current potential and the initial potential
        - basically (1) define some potential function based on the data structure (2) show that at no point does the ponetial drop below 0 (3) do this by showing the costs of the individual operations
    - dynamic table allocation  
        - a lot of hash table / maps need to grow when they get to large to ensure upper bound on load factor
        - this requires allocating a larger table and inserting elements into that table which takes a lot of time
        - despite this, the amortized cost of each operation is still O(1), we use amortized analysis to show this


---
chapter 22 - elementary graph algorithms
---
- algorithms

- concepts
    - intro
        - graph algorithms are the shit
        - fundamental to graphs is the notion of searching through a graph to gather information about it
        - we cover that
        - we also cover applications of that
    - graph representations
        - can rep either as adj list or adj graph
            - adj list if E << V^2
            - adj graph if E ~ V^2
        - adj list
            - this is an array where each element is indexed by a node, and that node has a pointer to a list of objects, where each object is a node connected to the original node
            - Theta(V + E) space
            - disadvantage is that it doesnt allow for quick lookup
        - adj mat
            - V^2 matrix where the vertices are randomly ordered and each entry contains a 1 if the corresponding vertices have an edge E between them
    - breadth first search 
        - method of searching a graph in such a way that a tree is constructed that (assuming equal edge costs) gives the shortest path between the starting vertex (source s) and all other nodes
        - it is called breadth-first search because it expands a frontier of vertices in such a manner that it discovers all vertices at distance k, before discovering any vertex of distance k + 1 from the source
        - tracks progress by coloring nodes
            - white = undiscovered
            - gray = has neighbors that are white
            - black = all neighbors discovered - i.e., all neighbors black or gray
        - establishes a bst tree that has expected protery of trees
        - the way it works is by keeping the nodes implicitly in 3 groups
            - unexplored (white)
            - nodes that have been explored (gray)
            - nodes that have had all their neighbors explored (black)
        - uses a queue to track exploration status
    -  depth first search
        - 


